{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m pairs \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m line \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Убедимся, что пары корректны\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m english_sentences, ukrainian_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mpairs[:\u001b[38;5;241m10000\u001b[39m])  \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# --- Токенізація ---\u001b[39;00m\n\u001b[1;32m     23\u001b[0m tokenizer_eng \u001b[38;5;241m=\u001b[39m Tokenizer(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- Завантаження даних ---\n",
    "PATH = 'ukr.txt'\n",
    "\n",
    "# Читання даних\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# Фильтрация строк: не пустые строки и наличие табуляции\n",
    "pairs = [line.strip().split('\\t') for line in data if '\\t' in line and len(line.strip().split('\\t')) == 2]\n",
    "\n",
    "# Убедимся, что пары корректны\n",
    "english_sentences, ukrainian_sentences = zip(*pairs[:10000])  \n",
    "\n",
    "# --- Токенізація ---\n",
    "tokenizer_eng = Tokenizer(filters='')\n",
    "tokenizer_ukr = Tokenizer(filters='')\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "tokenizer_ukr.fit_on_texts(ukrainian_sentences)\n",
    "\n",
    "eng_sequences = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "ukr_sequences = tokenizer_ukr.texts_to_sequences(ukrainian_sentences)\n",
    "\n",
    "eng_sequences = pad_sequences(eng_sequences, padding=\"post\")\n",
    "ukr_sequences = pad_sequences(ukr_sequences, padding=\"post\")\n",
    "\n",
    "# --- Параметри ---\n",
    "VOCAB_SIZE_ENG = len(tokenizer_eng.word_index) + 1\n",
    "VOCAB_SIZE_UKR = len(tokenizer_ukr.word_index) + 1\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "FF_DIM = 512\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# --- Модель трансформера ---\n",
    "def transformer_encoder(embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(None, embed_dim))\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    ffn_output = LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "    return Model(inputs, ffn_output)\n",
    "\n",
    "# --- Створення моделі ---\n",
    "def build_model():\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Ембединг\n",
    "    encoder_embedding = Embedding(VOCAB_SIZE_ENG, EMBED_DIM)(encoder_inputs)\n",
    "    decoder_embedding = Embedding(VOCAB_SIZE_UKR, EMBED_DIM)(decoder_inputs)\n",
    "\n",
    "    # Енкодер\n",
    "    encoder = transformer_encoder(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)(encoder_embedding)\n",
    "\n",
    "    # Декодер (простий варіант)\n",
    "    decoder = transformer_encoder(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)(decoder_embedding)\n",
    "    concat = tf.keras.layers.Concatenate()([encoder, decoder])\n",
    "\n",
    "    outputs = Dense(VOCAB_SIZE_UKR, activation=\"softmax\")(concat)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.summary()\n",
    "\n",
    "# --- Тренування ---\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "encoder_input_data = eng_sequences\n",
    "decoder_input_data = ukr_sequences[:, :-1]\n",
    "decoder_output_data = ukr_sequences[:, 1:]\n",
    "\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_output_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# --- Тест моделі ---\n",
    "def decode_sequence(input_seq):\n",
    "    encoder_output = model.predict([input_seq, tf.zeros_like(input_seq)])\n",
    "    predicted_sequence = tf.argmax(encoder_output, axis=-1).numpy()\n",
    "    return \" \".join([tokenizer_ukr.index_word.get(idx, \"<unk>\") for idx in predicted_sequence[0]])\n",
    "\n",
    "sample_sentence = \"How are you?\"\n",
    "sample_sequence = pad_sequences(tokenizer_eng.texts_to_sequences([sample_sentence]), maxlen=eng_sequences.shape[1])\n",
    "translation = decode_sequence(sample_sequence)\n",
    "print(\"Переклад:\", translation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
